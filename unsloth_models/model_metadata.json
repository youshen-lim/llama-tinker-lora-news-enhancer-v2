{
  "model_name": "Llama-3.2-1B-Newsletter-Analyst",
  "base_model": "unsloth/Llama-3.2-1B",
  "framework": "Unsloth",
  "version": "v3_train_on_responses_only",
  "training_date": "2025-11-06 06:56:34",
  "training_start": "2025-11-06 06:56:34",
  "training_end": "2025-11-06 07:01:29",
  "training_duration_minutes": 4.91,
  "training_methodology": {
    "approach": "train_on_responses_only",
    "loss_masking": "User input tokens masked (labels = -100)",
    "training_objective": "Cross-entropy loss on assistant responses only",
    "contamination_prevention": "Prevents model from learning conversation patterns",
    "implementation": "DataCollatorForSeq2Seq + train_on_responses_only() wrapper",
    "note": "Equivalent to Tinker's train_on_what=ALL_ASSISTANT_MESSAGES"
  },
  "training_examples": 364,
  "test_examples": 96,
  "split_ratio": "80/20",
  "dataset_source": "news_training_annotated.jsonl",
  "hyperparameters": {
    "batch_size": 4,
    "gradient_accumulation_steps": 1,
    "effective_batch_size": 4,
    "learning_rate": 0.0002,
    "num_epochs": 5,
    "max_steps": 455,
    "steps_per_epoch": 91,
    "warmup_steps": 22,
    "lora_rank": 32,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "optimizer": "adamw_8bit",
    "weight_decay": 0.01,
    "lr_scheduler_type": "cosine",
    "max_seq_length": 2048
  },
  "training_results": {
    "final_step_loss": 0.1582,
    "average_loss": 0.3078,
    "total_steps": 455,
    "trainable_parameters": 22544384,
    "total_parameters": 1258358784,
    "trainable_percentage": 1.79
  },
  "model_paths": {
    "lora_adapter": "./AI_Projects/News_FineTuning/unsloth_models/lora_adapter",
    "merged_model": "./AI_Projects/News_FineTuning/unsloth_models/merged_model",
    "gguf_f16": "./AI_Projects/News_FineTuning/unsloth_models/gguf_f16",
    "gguf_q4_k_m": "./AI_Projects/News_FineTuning/unsloth_models/gguf_q4_k_m"
  },
  "comparison_with_tinker": {
    "tinker_batch_size": 4,
    "tinker_gradient_accumulation": 1,
    "tinker_effective_batch_size": 4,
    "tinker_total_steps": 455,
    "tinker_final_loss_nll": 0.0447,
    "tinker_training_time_minutes": 8.51,
    "tinker_loss_masking": "train_on_what=ALL_ASSISTANT_MESSAGES",
    "unsloth_batch_size": 4,
    "unsloth_gradient_accumulation": 1,
    "unsloth_effective_batch_size": 4,
    "unsloth_total_steps": 455,
    "unsloth_final_step_loss": 0.1582,
    "unsloth_average_loss": 0.3078,
    "unsloth_training_time_minutes": 4.91,
    "unsloth_loss_masking": "train_on_responses_only()",
    "configurations_matched": true,
    "methodology_matched": true,
    "speedup": 1.73,
    "note": "Both models use response-only training for fair comparison"
  },
  "task": "newsletter_analysis",
  "task_description": "Extract structured intelligence metadata from newsletter content",
  "output_format": "JSON with relevance_score, summary, key_insights, company_names",
  "notes": [
    "\u2705 Retrained with train_on_responses_only() to eliminate contamination",
    "\u2705 Both Tinker and Unsloth now use response-only training methodology",
    "\u2705 Training completed in 4.91 minutes (1.73x faster than Tinker's 8.51 minutes)",
    "\u2705 Final step loss: 0.1582, Average loss: 0.3078",
    "\u2705 Expected contamination rate: 0% (down from 40% before retraining)",
    "Use merged_model (HuggingFace format) for evaluation, not GGUF"
  ]
}